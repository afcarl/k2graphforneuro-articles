 <!DOCTYPE html>
<html>
	<head>
		<title>K^2 Graph For Neuroevolution: Part 1</title>
		<link rel="stylesheet" href="style.css">
	</head>

	<body>

		<div class="container">

			<div class="main">
				<h1>K<sup>2</sup> Graph For Neuroevolution: Part 1</h1>
				<span class="author">By Stefano Palmieri</span>
			</div>

			<div class="main graphic" style="max-width: 60%">
			    <a href="https://plot.ly/~stefano.r.palmieri/189/?share_key=d9yWNeteioHyffBPewpzsk" target="_blank" title="plot from API" style="display: block; text-align: center;"><img src="https://plot.ly/~stefano.r.palmieri/189.png?share_key=d9yWNeteioHyffBPewpzsk" alt="plot from API" style="max-width: 60%;"  width="600" onerror="this.onerror=null;this.src='https://plot.ly/404.png';" /></a>
			    <script data-plotly="stefano.r.palmieri:189" sharekey-plotly="d9yWNeteioHyffBPewpzsk" src="https://plot.ly/embed.js" async></script>

			    <span class="desc">Neural network phenotype of a K<sup>2</sup> genome.</span>
			</div>


			<p class="main">This series will cover the K<sup>2</sup> Graph, a novel indirect encoding
			for neuroevolution. The reader should have a strong understanding of the NEAT algorithm by Kenneth Stanley and HyperNEAT before continuing. The research papers introducing these algorithms are quite accessible and don't require advanced mathematical knowledge. I highly recommend reading through them a couple of times.
			</p>


			<ul class="main">
				<li><a href="http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf">Evolving Neural Networks through Augmenting Topologies</a></li>
				<li><a href="http://eplex.cs.ucf.edu/papers/stanley_alife09.pdf">A Hypercube-Based Indirect Encoding for Evolving Large-Scale
				Neural Networks</a></li>
				<li><a href="http://eplex.cs.ucf.edu/hyperNEATpage/">HyperNEAT User's Page</a></li>
			</ul>

			<p class="main">The K<sup>2</sup> is an indirect encoding taking cues from HyperNEAT. 
			Like HyperNEAT and its variants, it aims to generate phenotypes utilizing biologically inspired principles such as <a href="https://www.youtube.com/watch?v=t15wUkCXuxQ">repetition, regularity, symmetry, hierarchy, and modularity.</a> In addition, it is designed to use an evolvable substrate, meaning that the number of neurons can grow indefinitely and that the neural architecture is not fixed or chosen by a human programmer. Thus it is more an alternative to Evolvable Substrate HyperNEAT (ES-HyperNEAT) than to vanilla HyperNEAT.</p>

			<p class="main">The K<sup>2</sup> Graph was designed with the intent of simplifying indirect encoding schemes conceptually and for implementation. It's matrix-based design lends itself to be implemented in modern tensor-based deep learning frameworks. Future posts will contain implementation details using Numpy, a popular Python library for scientific computing.</p>

			<h2 class="main">Adjacency Matrices</h2>

			<p class="main">Let's imagine a set of neurons connected by directed weights. A neural network can be described in mathematical terms as a directed, weighted graph where the
			neurons are the nodes of the graph and the weights are the edges.</p>

			<img class="main" src="Artificial_neural_network.svg" height=400rem>

			<span class="main attr">Illustration of a Neural Network by CBurnett licensed under <a href="https://creativecommons.org/licenses/by-sa/3.0/">CC BY-SA 3.0</a></span>

			<p class="main">A graph can be written in matrix form as an adjacency matrix. An adjacency matrix is a representation of a graph that is a type of square matrix where each element represents an edge. The correspending nodes connected by a weight are indicated by the row and column of the edge in the matrix.</p>

			<img class="main" src="adjacency_matrix.png">

			<span class="main attr">A weighted directed graph and its corresponding adjacency matrix.</span>

			<p class="main">The K<sup>2</sup> Graph heavily utilizes adjacency matrices and can be viewed as a nesting of adjacency matrices. </p>
			<!--
			Added bonus of sparsity, something that HyperNEAT has not been successful at. Sparsity is a likely ingredient for making deeper evolved networks. Sebation Risi showed that adding more layers to HyperNEAT doesn't improve performance, in face it descreases it.</p>-->
			<h2 class="main">The K<sup>2</sup> Tree</h2>

			<p class="main">The K<sup>2</sup> Tree is tree structure originally designed for efficiently compressing web graphs [1]. It represents an adjacency matrix as a tree structure where if a node contains a 1, it's subtree also contains a one and if a node contains a 0, the node is a leaf and has no subtree. All internal nodes have  k<sup>2</sup> children.</p>

			<img class="main" src="k2tree.svg">

			<span class="main attr">An example of a K<sup>2</sup> tree where k = 2.</span>


			<p class="main">To go from the K<sup>2</sup> Tree to the K<sup>2</sup> Graph some generalizations will need to be made.</p>

			<h2 class="main">Going from Tree to Graph</h2>

			<p class="main">The first necessary change to make K<sup>2</sup> Tree into K<sup>2</sup> Graph is the addition of real numbers. For the K<sup>2</sup> Graph, real numbers can exist in the nodes <i>and</i> the links. Let's start with a simple example. Note that there is some restructing here to help make the difference between nodes and a links more explicit by packaging the nodes into matrices of k by k.</p>

			<img class="main" src="k2graph1.png">

			<span class="main attr">An simple example of a K<sup>2</sup> graph, again where k = 2.</span>


			<p class="main">Giving values to the links gives allows for more expressive power so that different sub-matrices can have different weightings, which will be important for evolution.</p>

			<p class="main">Graph structures differ from tree structures in that they allow reuse of nodes. The K<sup>2</sup> graph will thus be a Directed Acyclic Graph (DAG). In this example, the lower node is referred to twice but with a different weighting (0.5 and 0.6) each time.</p>

			<img class="main" src="k2graph2.png">

			<span class="main attr">K<sup>2</sup> graph with node reuse.</span>

			<p class="main">Take a look at the picture below. Can you image a way to use a K<sup>2</sup> Graph to roughly compress the adjacency matrix? Are there any symmetries or patterns in the adjacency matrix that can be exploited?</p>

			<img class="main" style="width: 700px" src="https://upload.wikimedia.org/wikipedia/commons/c/c0/Average_Regional_Connection_Matrix,_Network_Layout,_and_Connectivity_Backbone.png">

			<span class="main attr">Average Regional Connection Matrix of the Human Brain by Patric Hagmann, Leila Cammoun, Xavier Gigandet, Reto Meuli, Christopher J. Honey, Van J. Wedeen, Olaf Sporns licensed under <a href="https://creativecommons.org/licenses/by/2.5/deed.en">CC Generic 2.5</a></span>

			<p class="main">In the next post, we'll see how K<sup>2</sup> Graphs can be expressed as genes.</p>

			<a class="main" href="https://stefanopalmieri.github.io/k2graphforneuro-articles/part2.html">Part 2 of this series</a>

			<h2 class="main">References</h2>

			<p class="main">
			1. Brisaboa, N.R., Ladra, S., Navarro, G.: k2-trees for compact web graph representation. In: Karlgren, J., Tarhio, J., Hyyro, H. (eds.) SPIRE 2009. LNCS, vol. 5721, pp. 18-30. Springer, Heidelberg (2009)
			</p>

		</div>

	</body>
</html> 
